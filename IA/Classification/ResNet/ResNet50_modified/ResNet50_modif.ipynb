{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatgement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet import ResNet50\n",
    "from keras.preprocessing import image\n",
    "import keras.utils as image\n",
    "from keras.applications.resnet import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions pour diviser le dataset entier en 80% de train et 20% de test de manière aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __copyfiles__(_folder, dataset_src,_class_, train_or_test_folder):\n",
    "    for _filename in _folder:\n",
    "            src_path = os.path.join(dataset_src,_class_, _filename)\n",
    "            dst_path = os.path.join(train_or_test_folder,_class_, _filename)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "def create_folder_and_class(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "            \n",
    "def split_data(src_folder, split_ratio):\n",
    "    print('\\n\\n\\n','First we have to create a new dataset which will contain the datas of train (80%) and test(20%) separated randomly')\n",
    "    new_dataset=input('Give a name for the new dataset : ')\n",
    "    train_folder=os.path.join(new_dataset,'Train')\n",
    "    test_folder=os.path.join(new_dataset,'Test')\n",
    "    create_folder_and_class(train_folder)\n",
    "    create_folder_and_class(test_folder)\n",
    "\n",
    "    src_folder_dir=os.listdir(src_folder)\n",
    "    for _class_ in src_folder_dir:\n",
    "        create_folder_and_class(os.path.join(train_folder,_class_))\n",
    "        create_folder_and_class(os.path.join(test_folder,_class_))\n",
    "        filenames = os.listdir(os.path.join(src_folder,_class_))\n",
    "        random.shuffle(filenames)\n",
    "        num_train = int(split_ratio * len(filenames))\n",
    "        train_filenames = filenames[:num_train]\n",
    "        test_filenames = filenames[num_train:]\n",
    "        __copyfiles__(train_filenames, src_folder,_class_, train_folder)\n",
    "        __copyfiles__(test_filenames, src_folder,_class_, test_folder)\n",
    "    \n",
    "    return train_folder, test_folder\n",
    "\n",
    "train_folder, test_folder = split_data('DataResize', 0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "# Load the ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model's layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a fully connected layer with a softmax activation\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the full model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonction de process des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(img):\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Define the data generators\n",
    "datagen = ImageDataGenerator(preprocessing_function=preprocess_function)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"resnet_model.h5\", monitor='accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "early = EarlyStopping(monitor='accuracy', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "callbacks_list = [checkpoint, early, reduce_lr]\n",
    "\n",
    "train_folder, test_folder = split_data('Datas', 0.8)\n",
    "\n",
    "X_train=datagen.flow_from_directory(train_folder,\n",
    "                                    target_size=(224,224),\n",
    "                                    batch_size=32,\n",
    "                                    class_mode='categorical')\n",
    "X_test=datagen.flow_from_directory(test_folder,\n",
    "                                   target_size=(224,224),\n",
    "                                   batch_size=32,\n",
    "                                   class_mode='categorical')\n",
    "# Train the model\n",
    "history = model.fit(X_train,epochs=20,\n",
    "                    validation_data=X_test,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracé des courbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "# Plot the accuracy and loss curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(acc, 'r', linewidth=2.0)\n",
    "plt.plot(val_acc, 'b', linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('Accuracy Curves', fontsize=16)\n",
    "plt.savefig('accuracy_res50_modif.png')\n",
    "\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(loss, 'r', linewidth=2.0)\n",
    "plt.plot(val_loss, 'b', linewidth=2.0)\n",
    "plt.legend(['Training Loss', 'Validation Loss'], fontsize=18)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Loss Curves', fontsize=16)\n",
    "plt.savefig('loss_res50_modif.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
